{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# use huggingface for data\n",
    "dataset_small = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "dataset_large = load_dataset(\"wikitext\", \"wikitext-103-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset is structured like this\n",
    "# top level: train, test, validate\n",
    "# second level: examples\n",
    "# third level: {'text': '...'}\n",
    "print(dataset[\"train\"][4]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass(repr=True)\n",
    "class Word2VecParams:\n",
    "    \"\"\" \n",
    "    Class contains all the configurable parameters\n",
    "    \"\"\"\n",
    "    # skipgram params\n",
    "    MIN_FREQ = 50  # frequency cutoff for vocabulary\n",
    "    SKIPGRAM_N_WORDS = 8  # number of neighboring words on skipgram\n",
    "    T = 85  # distribution percentile for sampling from vocab\n",
    "    NEG_SAMPLES = 50  # negative samples per training example\n",
    "    NS_ARRAY_LEN = 5_000_000  # negative sample vector size\n",
    "    SPECIALS = \"\"  # placeholder for words excluded due to low freq\n",
    "    TOKENIZER = \"basic_english\"  # e.g. split text by spaces\n",
    "\n",
    "    # network params\n",
    "    BATCH_SIZE = 100  # number of documents per batch\n",
    "    EMBED_DIM = 300  # embeddings dimension\n",
    "    N_EPOCHS = 5  # training epochs\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CRITERION = nn.BCEWithLogitsLoss()  # loss function\n",
    "\n",
    "\n",
    "params = Word2VecParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, List, Optional\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, list, specials):\n",
    "        # stoi= string-to-int, v[0]= word, k= index, v[1]= frequency\n",
    "        self.stoi = {v[0]: (k, v[1]) for k, v in enumerate(list)}\n",
    "        self.itos = {k: (v[0], v[1]) for k, v in enumerate(list)}\n",
    "        self._specials = specials[0]\n",
    "        # destructures stoi to add all frequencies for total vocab size\n",
    "        self.total_tokens = np.nansum([f for _, (_, f) in self.stoi.items()], dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi) - 1\n",
    "\n",
    "    def get_index(self, word: Union[str, List[str]]):\n",
    "        # single word rather than list\n",
    "        if isinstance(word, str):\n",
    "            # word is in vocabulary\n",
    "            if word in self.stoi:\n",
    "                # return key for token\n",
    "                return self.stoi.get(word)[0]\n",
    "            else:\n",
    "                # return key for out of vocab tokens\n",
    "                return self.stoi.get(self._specials)[0]\n",
    "        # dealing with a list of words, likely the skipgram\n",
    "        elif isinstance(word, list):\n",
    "            res = []\n",
    "            # do the same as for word, but for a list\n",
    "            # definitely a way to compact this code\n",
    "            for w in word:\n",
    "                if w in self.stoi:\n",
    "                    # if word in vocab add id to response\n",
    "                    res.append(self.stoi.get(w)[0])\n",
    "                else:\n",
    "                    # add out of vocab token id\n",
    "                    res.append(self.stoi.get(self._specials)[0])\n",
    "            return res\n",
    "        else:\n",
    "            raise ValueError(f\"Word {word} is not a string or list\")\n",
    "\n",
    "    def get_freq(self, word: Union[str, List[str]]):\n",
    "        if isinstance(word, str):\n",
    "            if word in self.stoi:\n",
    "                # return word frequency\n",
    "                return self.stoi.get(word)[1]\n",
    "            else:\n",
    "                # return frequency of non-vocab words\n",
    "                return self.stoi.get(self._specials)[1]\n",
    "        elif isinstance(word, list):\n",
    "            res = []\n",
    "            for w in word:\n",
    "                if w in self.stoi:\n",
    "                    res.append(self.stoi.get(w)[1])\n",
    "                else:\n",
    "                    res.append(self.stoi.get(self._specials)[1])\n",
    "            return res\n",
    "        else:\n",
    "            raise ValueError(f\"Word {word} is not a string or list\")\n",
    "\n",
    "    def lookup_token(self, token: Union[int, List[int]]):\n",
    "        if isinstance(token, (int, np.int64)):\n",
    "            if token in self.itos:\n",
    "                return itos.get(token)[0]\n",
    "            else:\n",
    "                raise ValueError(\"Token {token} out of vocabulary\")\n",
    "        elif isinstance(token, list):\n",
    "            res = []\n",
    "            for t in token:\n",
    "                if t in self.itos:\n",
    "                    res.append(self.itos.get(t)[0])\n",
    "                else:\n",
    "                    raise ValueError(f\"Token {t} is not a valid index\")\n",
    "            return res\n",
    "        else:\n",
    "            raise ValueError(f\"Token {token} is not an int or list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "\n",
    "def yield_tokens(iterator, tokenizer):\n",
    "    r = re.compile(\"[a-z1-9]\")\n",
    "    for i in iterator:\n",
    "        res = tokenizer(i['text'])\n",
    "        res = list(filter(r.match, res))\n",
    "        yield res\n",
    "\n",
    "\n",
    "def build_vocab(\n",
    "    iterator,\n",
    "    tokenizer,\n",
    "    params: Word2VecParams,\n",
    "    max_tokens: Optional[int] = None,\n",
    "):\n",
    "    counter = Counter()\n",
    "    for tokens in yield_tokens(iterator, tokenizer):\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # sort by freq descending, then lexicographically\n",
    "    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "    tokens = []\n",
    "    for token, freq in ordered_dict.items():\n",
    "        if freq >= params.MIN_FREQ:\n",
    "            tokens.append((token, freq))\n",
    "\n",
    "    specials = (params.SPECIALS, np.nan)\n",
    "    tokens[0] = specials\n",
    "\n",
    "    return Vocab(tokens, specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/timekeeper/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "vocab = build_vocab(dataset['train'], word_tokenize, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(vocab.stoi.get('Had'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the <unk> Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \\n'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
